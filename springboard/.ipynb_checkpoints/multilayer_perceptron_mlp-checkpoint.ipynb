{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Author: Raymundo Cassani\n",
    "April 2017\n",
    "This file contains the Multi-Layer Perceptron (MLP) class which creates a\n",
    "fully-connected-feedforward-artifitial-neural-network object with methods\n",
    "for its usage\n",
    "Methods:\n",
    "    __init__()\n",
    "    train(X, y, iterations, reset)\n",
    "    predict(X)\n",
    "    initialize_theta_weights()\n",
    "    backpropagation(X, Y)\n",
    "    feedforward(X)\n",
    "    unroll_weights(rolled_data)\n",
    "    roll_weights(unrolled_data)\n",
    "    sigmoid(z)\n",
    "    relu(z)\n",
    "    sigmoid_derivative(z)\n",
    "    relu_derivative(z)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class Mlp():\n",
    "    '''\n",
    "    fully-connected Multi-Layer Perceptron (MLP)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size_layers, act_funct='sigmoid', reg_lambda=0, bias_flag=True):\n",
    "        '''\n",
    "        Constructor method. Defines the characteristics of the MLP\n",
    "        Arguments:\n",
    "            size_layers : List with the number of Units for:\n",
    "                [Input, Hidden1, Hidden2, ... HiddenN, Output] Layers.\n",
    "            act_funtc   : Activation function for all the Units in the MLP\n",
    "                default = 'sigmoid'\n",
    "            reg_lambda: Value of the regularization parameter Lambda\n",
    "                default = 0, i.e. no regularization\n",
    "            bias: Indicates is the bias element is added for each layer, but the output\n",
    "        '''\n",
    "        self.size_layers = size_layers\n",
    "        self.n_layers    = len(size_layers)\n",
    "        self.act_f       = act_funct\n",
    "        self.lambda_r    = reg_lambda\n",
    "        self.bias_flag   = bias_flag\n",
    " \n",
    "        # Ramdomly initialize theta (MLP weights)\n",
    "        self.initialize_theta_weights()\n",
    "\n",
    "    def train(self, X, Y, iterations=400, reset=False):\n",
    "        '''\n",
    "        Given X (feature matrix) and y (class vector)\n",
    "        Updates the Theta Weights by running Backpropagation N tines\n",
    "        Arguments:\n",
    "            X          : Feature matrix [n_examples, n_features]\n",
    "            Y          : Sparse class matrix [n_examples, classes]\n",
    "            iterations : Number of times Backpropagation is performed\n",
    "                default = 400\n",
    "            reset      : If set, initialize Theta Weights before training\n",
    "                default = False\n",
    "        '''\n",
    "        n_examples = Y.shape[0]\n",
    "#        self.labels = np.unique(y)\n",
    "#        Y = np.zeros((n_examples, len(self.labels)))\n",
    "#        for ix_label in range(len(self.labels)):\n",
    "#            # Find examples with with a Label = lables(ix_label)\n",
    "#           ix_tmp = np.where(y == self.labels[ix_label])[0]\n",
    "#            Y[ix_tmp, ix_label] = 1\n",
    "\n",
    "        if reset:\n",
    "            self.initialize_theta_weights()\n",
    "        for iteration in range(iterations):\n",
    "            self.gradients = self.backpropagation(X, Y)\n",
    "            self.gradients_vector = self.unroll_weights(self.gradients)\n",
    "            self.theta_vector = self.unroll_weights(self.theta_weights)\n",
    "            self.theta_vector = self.theta_vector - self.gradients_vector\n",
    "            self.theta_weights = self.roll_weights(self.theta_vector)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Given X (feature matrix), y_hay is computed\n",
    "        Arguments:\n",
    "            X      : Feature matrix [n_examples, n_features]\n",
    "        Output:\n",
    "            y_hat  : Computed Vector Class for X\n",
    "        '''\n",
    "        A , Z = self.feedforward(X)\n",
    "        Y_hat = A[-1]\n",
    "        return Y_hat\n",
    "\n",
    "    def initialize_theta_weights(self):\n",
    "        '''\n",
    "        Initialize theta_weights, initialization method depends\n",
    "        on the Activation Function and the Number of Units in the current layer\n",
    "        and the next layer.\n",
    "        The weights for each layer as of the size [next_layer, current_layer + 1]\n",
    "        '''\n",
    "        self.theta_weights = []\n",
    "        size_next_layers = self.size_layers.copy()\n",
    "        size_next_layers.pop(0)\n",
    "        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "            if self.act_f == 'sigmoid':\n",
    "                # Method presented \"Understanding the difficulty of training deep feedforward neurla networks\"\n",
    "                # Xavier Glorot and Youshua Bengio, 2010\n",
    "                epsilon = 4.0 * np.sqrt(6) / np.sqrt(size_layer + size_next_layer)\n",
    "                # Weigts from a uniform distribution [-epsilon, epsion]\n",
    "                if self.bias_flag:  \n",
    "                    theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer + 1) * 2.0 ) - 1)\n",
    "                else:\n",
    "                    theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer) * 2.0 ) - 1)            \n",
    "            elif self.act_f == 'relu':\n",
    "                # Method presented in \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication\"\n",
    "                # He et Al. 2015\n",
    "                epsilon = np.sqrt(2.0 / (size_layer * size_next_layer) )\n",
    "                # Weigts from Normal distribution mean = 0, std = epsion\n",
    "                if self.bias_flag:\n",
    "                    theta_tmp = epsilon * (np.random.randn(size_next_layer, size_layer + 1 ))\n",
    "                else:\n",
    "                    theta_tmp = epsilon * (np.random.randn(size_next_layer, size_layer))                    \n",
    "            self.theta_weights.append(theta_tmp)\n",
    "        return self.theta_weights\n",
    "\n",
    "    def backpropagation(self, X, Y):\n",
    "        '''\n",
    "        Implementation of the Backpropagation algorithm with regularization\n",
    "        '''\n",
    "        if self.act_f == 'sigmoid':\n",
    "            g_dz = lambda x: self.sigmoid_derivative(x)\n",
    "        elif self.act_f == 'relu':\n",
    "            g_dz = lambda x: self.relu_derivative(x)\n",
    "\n",
    "        n_examples = X.shape[0]\n",
    "        # Feedforward\n",
    "        A, Z = self.feedforward(X)\n",
    "\n",
    "        # Backpropagation\n",
    "        deltas = [None] * self.n_layers\n",
    "        deltas[-1] = A[-1] - Y\n",
    "        # For the second last layer to the second one\n",
    "        for ix_layer in np.arange(self.n_layers - 1 - 1 , 0 , -1):\n",
    "            theta_tmp = self.theta_weights[ix_layer]\n",
    "            if self.bias_flag:\n",
    "                # Removing weights for bias\n",
    "                theta_tmp = np.delete(theta_tmp, np.s_[0], 1)\n",
    "            deltas[ix_layer] = (np.matmul(theta_tmp.transpose(), deltas[ix_layer + 1].transpose() ) ).transpose() * g_dz(Z[ix_layer])\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = [None] * (self.n_layers - 1)\n",
    "        for ix_layer in range(self.n_layers - 1):\n",
    "            grads_tmp = np.matmul(deltas[ix_layer + 1].transpose() , A[ix_layer])\n",
    "            grads_tmp = grads_tmp / n_examples\n",
    "            if self.bias_flag:\n",
    "                # Regularize weights, except for bias weigths\n",
    "                grads_tmp[:, 1:] = grads_tmp[:, 1:] + (self.lambda_r / n_examples) * self.theta_weights[ix_layer][:,1:]\n",
    "            else:\n",
    "                # Regularize ALL weights\n",
    "                grads_tmp = grads_tmp + (self.lambda_r / n_examples) * self.theta_weights[ix_layer]       \n",
    "            gradients[ix_layer] = grads_tmp;\n",
    "        return gradients\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        '''\n",
    "        Implementation of the Feedforward\n",
    "        '''\n",
    "        if self.act_f == 'sigmoid':\n",
    "            g = lambda x: self.sigmoid(x)\n",
    "        elif self.act_f == 'relu':\n",
    "            g = lambda x: self.relu(x)\n",
    "\n",
    "        A = [None] * self.n_layers\n",
    "        Z = [None] * self.n_layers\n",
    "        input_layer = X\n",
    "\n",
    "        for ix_layer in range(self.n_layers - 1):\n",
    "            n_examples = input_layer.shape[0]\n",
    "            if self.bias_flag:\n",
    "                # Add bias element to every example in input_layer\n",
    "                input_layer = np.concatenate((np.ones([n_examples ,1]) ,input_layer), axis=1)\n",
    "            A[ix_layer] = input_layer\n",
    "            # Multiplying input_layer by theta_weights for this layer\n",
    "            Z[ix_layer + 1] = np.matmul(input_layer,  self.theta_weights[ix_layer].transpose() )\n",
    "            # Activation Function\n",
    "            output_layer = g(Z[ix_layer + 1])\n",
    "            # Current output_layer will be next input_layer\n",
    "            input_layer = output_layer\n",
    "\n",
    "        A[self.n_layers - 1] = output_layer\n",
    "        return A, Z\n",
    "\n",
    "\n",
    "    def unroll_weights(self, rolled_data):\n",
    "        '''\n",
    "        Unroll a list of matrices to a single vector\n",
    "        Each matrix represents the Weights (or Gradients) from one layer to the next\n",
    "        '''\n",
    "        unrolled_array = np.array([])\n",
    "        for one_layer in rolled_data:\n",
    "            unrolled_array = np.concatenate((unrolled_array, one_layer.flatten(1)) )\n",
    "        return unrolled_array\n",
    "\n",
    "    def roll_weights(self, unrolled_data):\n",
    "        '''\n",
    "        Unrolls a single vector to a list of matrices\n",
    "        Each matrix represents the Weights (or Gradients) from one layer to the next\n",
    "        '''\n",
    "        size_next_layers = self.size_layers.copy()\n",
    "        size_next_layers.pop(0)\n",
    "        rolled_list = []\n",
    "        if self.bias_flag:\n",
    "            extra_item = 1\n",
    "        else:\n",
    "            extra_item = 0\n",
    "        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "            n_weights = (size_next_layer * (size_layer + extra_item))\n",
    "            data_tmp = unrolled_data[0 : n_weights]\n",
    "            data_tmp = data_tmp.reshape(size_next_layer, (size_layer + extra_item), order = 'F')\n",
    "            rolled_list.append(data_tmp)\n",
    "            unrolled_data = np.delete(unrolled_data, np.s_[0:n_weights])\n",
    "        return rolled_list\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "        Sigmoid function\n",
    "        z can be an numpy array or scalar\n",
    "        '''\n",
    "        result = 1.0 / (1.0 + np.exp(-z))\n",
    "        return result\n",
    "\n",
    "    def relu(self, z):\n",
    "        '''\n",
    "        Rectified Linear function\n",
    "        z can be an numpy array or scalar\n",
    "        '''\n",
    "        if np.isscalar(z):\n",
    "            result = np.max((z, 0))\n",
    "        else:\n",
    "            zero_aux = np.zeros(z.shape)\n",
    "            meta_z = np.stack((z , zero_aux), axis = -1)\n",
    "            result = np.max(meta_z, axis = -1)\n",
    "        return result\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        '''\n",
    "        Derivative for Sigmoid function\n",
    "        z can be an numpy array or scalar\n",
    "        '''\n",
    "        result = self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "        return result\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        '''\n",
    "        Derivative for Rectified Linear function\n",
    "        z can be an numpy array or scalar\n",
    "        '''\n",
    "        result = 1 * (z > 0)\n",
    "        return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
